{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# config, generator, discriminator, dataset done\n# utils, train to do","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#config\nimport torch\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nconfig = {\n'DEVICE' : 'cuda',\n'TRAIN_DIR' :  '../input/pix2pix-dataset/',\n'VAL_DIR' : '../input/pix2pix-dataset/',\n'LEARNING_RATE' : 2e-4,\n'BATCH_SIZE' : 16,\n'NUM_WORKERS' : 2,\n'IMAGE_SIZE' : 256,\n'CHANNELS_IMG' : 3,\n'L1_LAMBDA' : 100,\n'LAMBDA_GP' : 10,\n'NUM_EPOCHS' : 100,\n'LOAD_MODEL' : False,\n'SAVE_MODEL' :  False,\n'CHECKPOINT_DISC' : './discriminator',\n'CHECKPOINT_GEN' : './generator'}\n\nboth_transform = A.Compose([A.Resize(width=256, height=256)], additional_targets={'image0':'image'})\ntransform_only_input = A.Compose([A.HorizontalFlip(0.5),\n                                  A.ColorJitter(p=0.2),\n                                  A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5],max_pixel_value=255.0),\n                                 ToTensorV2()])\ntransform_only_mask = A.Compose([A.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5], max_pixel_value=255.0),\n                                ToTensorV2()])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:41:02.786352Z","iopub.execute_input":"2022-04-02T13:41:02.786787Z","iopub.status.idle":"2022-04-02T13:41:02.802306Z","shell.execute_reply.started":"2022-04-02T13:41:02.786741Z","shell.execute_reply":"2022-04-02T13:41:02.801223Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# dataset\nimport numpy as np\nfrom PIL import Image\nimport os\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.utils import save_image\n\n\nclass MapDataSet(Dataset):\n    def __init__(self, path):\n        self.path = path\n        self.list_files = os.listdir(self.path)\n        \n    def __len__(self):\n        return len(self.list_files)\n    \n    def __getitem__(self, index):\n        img_file = self.list_files[index]\n        img_path = os.path.join(self.path, img_file)\n        image = np.array(Image.open(img_path))\n        input_image = image[:, :600, :]\n        target_image = image[:, 600:, :]\n        \n        augmentations = both_transform(image=input_image, image0=target_image)\n        input_image = augmentations['image']\n        target_image = augmentations['image0']\n        \n        input_image = transform_only_input(image=input_image)['image']\n        target_image = transform_only_mask(image=target_image)['image']\n        \n        return input_image, target_image\n        \nif __name__ == '__main__':\n    dataset = MapDataSet('../input/pix2pix-dataset/maps/maps/train')\n    loader = DataLoader(dataset, batch_size=5)\n#     for x, y in loader:\n#         print(x.shape)\n#         save_image(x, 'x.png')\n#         save_image(y, 'y.png')\n#         import sys\n#         sys.exit()\n      \n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:38:28.184454Z","iopub.execute_input":"2022-04-02T13:38:28.184710Z","iopub.status.idle":"2022-04-02T13:38:28.499211Z","shell.execute_reply.started":"2022-04-02T13:38:28.184682Z","shell.execute_reply":"2022-04-02T13:38:28.498232Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Discriminator\n# concatenated x and y because the input and output images are combined in a single image\n# also, that's why inp_channels*2 in Conv2d layer\nimport torch \nimport torch.nn as nn\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels,stride=2):\n        super(CNNBlock, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding=1, padding_mode='reflect'),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.2))\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass Discriminator(nn.Module):\n    \n    def __init__(self, in_channels, features=[64,128,256,512]): #256 -> 30x30\n        super().__init__()\n        self.initial = nn.Sequential(\n        nn.Conv2d(in_channels*2,features[0], kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n        nn.LeakyReLU(0.2)\n        )\n        \n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            layers.append(\n                CNNBlock(in_channels, feature, stride=1 if feature==features[-1] else 2),\n            )\n            in_channels = feature\n            \n        layers.append(\n            nn.Conv2d(\n                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n            ),\n        )\n        self.model = nn.Sequential(*layers)    \n    \n    def forward(self, x, y):\n        x = torch.cat([x,y], dim=1)\n        x = self.initial(x)\n        return self.model(x)\n    \ndef test():\n    x = torch.randn((1, 3, 256, 256))\n    y = torch.randn((1, 3, 256, 256))\n    model = Discriminator(in_channels=3)\n    preds = model(x, y)\n    print(model)\n    print(preds.shape)\n\n\nif __name__ == \"__main__\":\n    test()\n    \n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-02T13:38:31.184532Z","iopub.execute_input":"2022-04-02T13:38:31.184791Z","iopub.status.idle":"2022-04-02T13:38:31.472592Z","shell.execute_reply.started":"2022-04-02T13:38:31.184761Z","shell.execute_reply":"2022-04-02T13:38:31.471848Z"},"_kg_hide-input":false,"jupyter":{"source_hidden":true},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Discriminator(\n  (initial): Sequential(\n    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n    (1): LeakyReLU(negative_slope=0.2)\n  )\n  (model): Sequential(\n    (0): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (1): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (2): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n  )\n)\ntorch.Size([1, 1, 30, 30])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generator\n\n# Similar to UNet\n# down specifies downward part of UNet\n# Encoder LeakyReLU, decoder ReLU\n# dont use batchnorm in initial layer\n# bias=False because we are using BatchNorm\n# output image of init has dimensions 128, as (n-f+2p/s)+1 => (256-4+2)/2 + 1 => 128\n\nimport torch \nimport torch.nn as nn\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, act='relu', use_dropout=False):\n        super(Block, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, padding_mode='reflect')\n        if down\n        else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU() if act =='relu' else nn.LeakyReLU(0.2),   \n        )\n        \n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.down = down\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return self.dropout(x) if self.use_dropout else x\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, features=64):\n        super().__init__()\n        self.initial_down = nn.Sequential(\n            nn.Conv2d(in_channels,features, 4, 2, 1, padding_mode='reflect' ),\n            nn.LeakyReLU(0.2))\n        self.down1 = Block(features, features*2, down=True, act='leaky', use_dropout=False) #64x64\n        self.down2 = Block(features*2, features*4, down=True, act='leaky', use_dropout=False) #32x32\n        self.down3 = Block(features*4, features*8, down=True, act='leaky', use_dropout=False) #16x16\n        self.down4 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #8x8\n        self.down5 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #4x4\n        self.down6 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #2x2\n        \n        self.bottleneck = nn.Sequential(nn.Conv2d(features*8, features*8, 4, 2, 1), nn.ReLU()) #1x1\n        \n        self.up1 = Block(features*8, features*8, down=False, act='relu', use_dropout=True)\n        self.up2 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up3 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up4 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up5 = Block(features*8*2, features*4, down=False, act='relu', use_dropout=True)\n        self.up6 = Block(features*4*2, features*2, down=False, act='relu', use_dropout=True)\n        self.up7 = Block(features*2*2, features, down=False, act='relu', use_dropout=True)\n        self.final_up = nn.Sequential(nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),)\n        \n    def forward(self, x):\n        d1 = self.initial_down(x)\n        d2 = self.down1(d1)\n        d3 = self.down2(d2)\n        d4 = self.down3(d3)\n        d5 = self.down4(d4)\n        d6 = self.down5(d5)\n        d7 = self.down6(d6)\n        bottleneck = self.bottleneck(d7)\n        up1 = self.up1(bottleneck)\n        up2 = self.up2(torch.cat([up1, d7], 1))\n        up3 = self.up3(torch.cat([up2, d6], 1))\n        up4 = self.up4(torch.cat([up3, d5], 1))\n        up5 = self.up5(torch.cat([up4, d4], 1))\n        up6 = self.up6(torch.cat([up5, d3], 1))\n        up7 = self.up7(torch.cat([up6, d2], 1))\n        return self.final_up(torch.cat([up7, d1], 1))\ndef test():\n    x = torch.randn((1, 3, 256, 256))\n    model = Generator(in_channels=3, features=64)\n    preds = model(x)\n    print(preds.shape)\n\n\nif __name__ == \"__main__\":\n    test()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:38:34.260225Z","iopub.execute_input":"2022-04-02T13:38:34.260730Z","iopub.status.idle":"2022-04-02T13:38:35.084612Z","shell.execute_reply.started":"2022-04-02T13:38:34.260695Z","shell.execute_reply":"2022-04-02T13:38:35.083860Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"# utils\nimport torch\nfrom torchvision.utils import save_image\n\ndef save_some_exanples(gen, val_loader, epoch, folder):\n    x, y =next(iter(val_loader))\n    x, y = x.to(config['DEVICE']), y.to(config['DEVICE'])\n    gen.eval()\n    with torch.no_grad():\n        y_fake = gen(x)\n        y_fake = y_fake * 0.5 + 0.5 #removing normalization\n        save_image(y_fake, folder + f'/y_gen_{epoch}.png')\n        if epoch == 1:\n            save_image(y * 0.5 + 0.5, folder + f'/label_{epoch}.png' )\n    gen.train()        \n        \ndef save_checkpoint(model, optimizer, filename='my_checkpoint.pth.tar'):\n    print('=> Saving Checkpoint')\n    checkpoint = {\n        'state_dict': model.state_dict(),\n        'optimizer': optimizer.state_dict()\n    }\n    torch.save(checkpoint, filename)\n    \ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print('=> Loading checkpoint')\n    checkpoint = torch.load(checkpoint_file, map_location=config['DEVICE'])\n    model.load_state_dict(checkpoint['state_dict'])\n    optimizer.load_state_dict(checkpoint['optimizer'])\n    \n    # If we don't do this then it will just have learning rate of old checkpoint\n    # and it will lead to many hours of debugging \\:\n    for param_group in optimizer.param_groups:\n        param_group['lr'] = lr\n    ","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:38:37.837938Z","iopub.execute_input":"2022-04-02T13:38:37.838469Z","iopub.status.idle":"2022-04-02T13:38:37.847354Z","shell.execute_reply.started":"2022-04-02T13:38:37.838427Z","shell.execute_reply":"2022-04-02T13:38:37.846471Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# train\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom tqdm import tqdm\nfrom torchvision.utils import save_image\ntorch.backends.cudnn.benchmark = True\n\ndef train_fn(disc, gen, loader, opt_disc,opt_gen, l1_loss, bce, g_scaler, d_scaler):\n    loop = tqdm(loader, leave=True)\n    \n    for idx, (x,y) in enumerate(loop):\n        x = x.to(config[config['DEVICE']])\n        y = y.to(config['DEVICE'])\n        \n        #Train Discriminator\n        with torch.cuda.amp.autocast():\n            y_fake = gen(x)\n            D_real = disc(x,y)\n            D_real_loss = bce(D_real, torch.ones_like(D_real))\n            D_fake = disc(x, y_fake.detach())\n            D_fake_loss = bce(D_fake, torch.zeros_like(D_fake))\n            D_loss = (D_real_loss + D_fake_loss)/2            \n    \n        disc.zero_grad()\n        d_scaler.scale(D_loss).backward()\n        d_scaler.step(opt_disc)\n        d_scaler.update()\n        \n        #Train Generator\n        with torch.cuda.amp.autocast():\n            D_fake = disc(x, y_fake)\n            G_fake_loss = bce(D_fake, torch.ones_like(D_fake))\n            L1 = l1_loss(y_fake, y)*(config['L1_LAMBDA'])\n            G_loss = G_fake_loss + L1\n            \n        opt_gen.zero_grad()\n        g_scaler.scale(G_loss).backward()\n        g_scaler.step(opt_gen)\n        g_scaler.update()\n        \n        if idx % 10 == 0:\n            loop.set_postfix(\n            D_real=torch.sigmoid(D_real).mean().item(),\n            D_fake=torch.sigmoid(D_fake).mean().item())        \n# def main():\n\ndisc = Discriminator(in_channels=3).to(config['DEVICE'])\ngen = Generator(in_channels=3, features=64).to(config['DEVICE'])\nopt_disc = optim.Adam(disc.parameters(), lr=config['LEARNING_RATE'], betas=(0.5, 0.999))\nopt_gen = optim.Adam(gen.parameters(), lr=config['LEARNING_RATE'], betas=(0.5, 0.999))\nBCE = nn.BCEWithLogitsLoss()\nL1_LOSS = nn.L1Loss()\n\nif config['LOAD_MODEL']:\n    load_checkpoint(config['CHECKPOINT_GEN'], gen, opt_gen, config['LEARNING_RATE'])\n    load_checkpoint(config['CHECKPOINT_DISC'], disc, opt_disc, config['LEARNING_RATE'])\n\ntrain_dataset = MapDataSet(path=config['TRAIN_DIR'])\ntrain_loader = DataLoader( train_dataset, batch_size=config['BATCH_SIZE'], shuffle=True, num_workers=config['NUM_WORKERS'])\ng_scaler = torch.cuda.amp.GradScaler()\nd_scaler = torch.cuda.amp.GradScaler()\nval_dataset = MapDataSet(path=config['VAL_DIR'])\nval_loader = DataLoader(val_dataset, batch_size=1, shuffle=False)\n\nfor epoch in range(config['NUM_EPOCHS']):\n    train_fn(disc, gen, train_loader,opt_disc, opt_gen, L1_LOSS, BCE, g_scaler, d_scaler)\n    if config['SAVE_MODEL'] and epoch % 5 == 0:\n        save_checkpoint(gen, opt_gen, filename=config['CHECKPOINT_GEN'])\n        save_checkpoint(disc, opt_disc, filename=config['CHECKPOINT_DISC'])\n    save_some_examples(gen, val_loader, num_epoch, folder='evaluation')    \n# if __name__ == 'main':\n#     print('hi')\n#     main()          \n       ","metadata":{"execution":{"iopub.status.busy":"2022-04-02T13:43:49.726418Z","iopub.execute_input":"2022-04-02T13:43:49.726681Z","iopub.status.idle":"2022-04-02T13:43:50.389980Z","shell.execute_reply.started":"2022-04-02T13:43:49.726652Z","shell.execute_reply":"2022-04-02T13:43:50.388517Z"},"jupyter":{"source_hidden":true},"trusted":true},"execution_count":17,"outputs":[{"name":"stderr","text":"  0%|          | 0/1 [00:00<?, ?it/s]\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIsADirectoryError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_33/260379740.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'NUM_EPOCHS'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0mtrain_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopt_disc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mL1_LOSS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBCE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_scaler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_scaler\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'SAVE_MODEL'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m5\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0msave_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'CHECKPOINT_GEN'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_33/260379740.py\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(disc, gen, loader, opt_disc, opt_gen, l1_loss, bce, g_scaler, d_scaler)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mloop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mleave\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'DEVICE'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1180\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1181\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1201\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m                 \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_task_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1203\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1227\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_try_put_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1228\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1229\u001b[0;31m             \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreraise\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1230\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/opt/conda/lib/python3.7/site-packages/torch/_utils.py\u001b[0m in \u001b[0;36mreraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    423\u001b[0m             \u001b[0;31m# have message field\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    424\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 425\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIsADirectoryError\u001b[0m: Caught IsADirectoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_33/3852512083.py\", line 20, in __getitem__\n    image = np.array(Image.open(img_path))\n  File \"/opt/conda/lib/python3.7/site-packages/PIL/Image.py\", line 2912, in open\n    fp = builtins.open(filename, \"rb\")\nIsADirectoryError: [Errno 21] Is a directory: '../input/pix2pix-dataset/cityscapes'\n"],"ename":"IsADirectoryError","evalue":"Caught IsADirectoryError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/worker.py\", line 287, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/opt/conda/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/tmp/ipykernel_33/3852512083.py\", line 20, in __getitem__\n    image = np.array(Image.open(img_path))\n  File \"/opt/conda/lib/python3.7/site-packages/PIL/Image.py\", line 2912, in open\n    fp = builtins.open(filename, \"rb\")\nIsADirectoryError: [Errno 21] Is a directory: '../input/pix2pix-dataset/cityscapes'\n","output_type":"error"}]},{"cell_type":"code","source":"","metadata":{"jupyter":{"source_hidden":true}},"execution_count":null,"outputs":[]}]}