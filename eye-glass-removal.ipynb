{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Imports\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nfrom torchvision.utils import save_image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom PIL import Image, ImageOps\nimport time\n\ntrain = pd.read_csv('../input/glasses-or-no-glasses/train.csv')\ntest = pd.read_csv('../input/glasses-or-no-glasses/test.csv')\ntrain.set_index('id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T17:29:23.104920Z","iopub.execute_input":"2022-04-22T17:29:23.105348Z","iopub.status.idle":"2022-04-22T17:29:27.432832Z","shell.execute_reply.started":"2022-04-22T17:29:23.105311Z","shell.execute_reply":"2022-04-22T17:29:27.432009Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# configs \nBATCH_SIZE = 1\nLAMBDA_IDENTITY = 0.0\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 2\nLAMBDA_CYCLE = 10 \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:03.824635Z","iopub.execute_input":"2022-04-22T15:28:03.825315Z","iopub.status.idle":"2022-04-22T15:28:03.829897Z","shell.execute_reply.started":"2022-04-22T15:28:03.825284Z","shell.execute_reply":"2022-04-22T15:28:03.829180Z"},"trusted":true},"execution_count":36,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:04.186884Z","iopub.execute_input":"2022-04-22T15:28:04.187206Z","iopub.status.idle":"2022-04-22T15:28:04.193788Z","shell.execute_reply.started":"2022-04-22T15:28:04.187175Z","shell.execute_reply":"2022-04-22T15:28:04.192987Z"},"trusted":true},"execution_count":37,"outputs":[]},{"cell_type":"code","source":"# Output Directories\nresults_NoGlasses = './NoGlasses'\nresults_Glasses = './Glasses'","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:04.612290Z","iopub.execute_input":"2022-04-22T15:28:04.612871Z","iopub.status.idle":"2022-04-22T15:28:04.616988Z","shell.execute_reply.started":"2022-04-22T15:28:04.612829Z","shell.execute_reply":"2022-04-22T15:28:04.615954Z"},"trusted":true},"execution_count":38,"outputs":[]},{"cell_type":"code","source":"# segregating images with and without glasses\ndirectory = '../input/glasses-or-no-glasses/faces-spring-2020/faces-spring-2020'\ntrain_imgs_glasses = []\ntrain_imgs_no_glasses = []\ntest_imgs = []\nfor img in os.listdir(directory):\n    img_id = int(img.replace('face-', ''  ).replace('.png', ''))\n    for i in range(len(train)+1):\n        if (i) == (img_id):\n            if train.loc[img_id]['glasses'] == 0:\n                train_imgs_no_glasses.append(os.path.join(directory, ('face-' + str(img) + '.png')))\n                break\n                \n            elif train.loc[img_id]['glasses'] == 1:\n                train_imgs_glasses.append(os.path.join(directory, ('face-' + str(img) + '.png')))\n                break\n                \n        \n    for i in range(len(test)+1): \n        if int(i) == int(img_id):\n            test_imgs.append(os.path.join(directory, ('face-' + str(img) + '.png')))\n            break         ","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:05.040891Z","iopub.execute_input":"2022-04-22T15:28:05.041302Z","iopub.status.idle":"2022-04-22T15:28:11.100275Z","shell.execute_reply.started":"2022-04-22T15:28:05.041271Z","shell.execute_reply":"2022-04-22T15:28:11.099451Z"},"trusted":true},"execution_count":39,"outputs":[]},{"cell_type":"code","source":"print(len(train_imgs_glasses))\nprint(len(train_imgs_no_glasses))\nprint(len(test_imgs))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:11.101722Z","iopub.execute_input":"2022-04-22T15:28:11.101993Z","iopub.status.idle":"2022-04-22T15:28:11.107241Z","shell.execute_reply.started":"2022-04-22T15:28:11.101966Z","shell.execute_reply":"2022-04-22T15:28:11.106466Z"},"trusted":true},"execution_count":40,"outputs":[{"name":"stdout","text":"2856\n1644\n500\n","output_type":"stream"}]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self,imgs_glasses, img_no_glasses, transform = None ):\n        super().__init__()\n        self.root_dir = directory\n        self.glasses_list = imgs_glasses\n        self.no_glasses_list = img_no_glasses\n        self.transform = transform\n        \n    def __len__(self):\n        self.glasses_len = len(self.glasses_list)\n        self.no_glasses_len = len(self.no_glasses_list)\n        self.dataset_len = max(self.glasses_len, self.no_glasses_len)\n        return self.dataset_len\n    \n    def __getitem__(self, idx):\n        start_time = time.time()\n        glasses_path = self.glasses_list[idx % self.dataset_len]\n        no_glasses_paths = self.no_glasses_list[idx % self.dataset_len]\n        img_glasses = np.array(ImageOps.grayscale(Image.open(glasses_path).convert('RGB')))\n        img_no_glasses = np.array(ImageOps.grayscale(Image.open(no_glasses_path).convert('RGB')))\n        \n        if self.transform:\n            augmentation = self.transform(image=img_glasses, imgage0=img_no_glasses)\n            img_glasses = augmentation[\"image0\"]\n            img_no_glasses = augmentation[\"image\"]\n            \n            \n        end_time = time.time()    \n        print(f\"{end_time - start_time} data loading time\")\n        return img_glasses, img_no_glasses","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:11.108542Z","iopub.execute_input":"2022-04-22T15:28:11.109260Z","iopub.status.idle":"2022-04-22T15:28:11.121214Z","shell.execute_reply.started":"2022-04-22T15:28:11.109213Z","shell.execute_reply":"2022-04-22T15:28:11.120238Z"},"trusted":true},"execution_count":41,"outputs":[]},{"cell_type":"code","source":"transforms_train = A.Compose(\n    [\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5], std=[0.5], max_pixel_value=255.0),\n        ToTensorV2()\n    ],\n    additional_targets = {\"image0\":\"image\"}\n)\n\ntransforms_val = A.Compose(\n    [\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5], std=[0.5], max_pixel_value=255.0),\n        ToTensorV2()\n    ],\n    additional_targets = {\"image0\":\"image\"}\n)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:11.123460Z","iopub.execute_input":"2022-04-22T15:28:11.123947Z","iopub.status.idle":"2022-04-22T15:28:11.136260Z","shell.execute_reply.started":"2022-04-22T15:28:11.123885Z","shell.execute_reply":"2022-04-22T15:28:11.135507Z"},"trusted":true},"execution_count":42,"outputs":[]},{"cell_type":"code","source":"# Generator\n\nclass SelfAttentionBlock(nn.Module):\n    \n    def __init__(self, in_dim):\n        super().__init__()\n        self.in_channel = in_dim\n        self.query_conv = nn.Conv2d(\n            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1\n        )\n        self.key_conv = nn.Conv2d(\n            in_channels=in_dim, out_channels=in_dim//8, kernel_size=1\n        )\n        self.value_conv = nn.Conv2d(\n            in_channels=in_dim, out_channels=in_dim, kernel_size=1\n        )\n        self.gamma = nn.Parameter(torch.zeros(1))\n        self.softmax = nn.Softmax(dim=-1)\n    \n    def forward():\n        \"\"\"\n        inputs:\n            x : input feature maps (B X C X W X H)\n        returns:\n            out : self attention value + input features\n            attention: B x N X N (N is Width*Height)\n        \"\"\"\n        batch_size, c, h, w = x.shape\n        proj_query = self.query_conv(x).view(batch_size, -1, w*h),permute(0, 2, 1)\n        # proj_query without permute has a shape of (batch_size, c//8, w*h)\n        # proj_query after permute has a shape of (batch_size, w*h, c//8)\n        # this is done in order to make proj_query compatible for matrix mult with proj_key\n        proj_key = self.key_conv(x).view(batch_size, -1, w*h) \n        \n        energy = torch.bmm(proj_query, proj_key) #(batch_size, w*h, w*h)\n        attention = self.softmax(energy) #(batch_size, w*h, w*h)\n        \n        proj_value = self.value_conv(x).view(batch_size, -1, w*h)  # (batch_size, c, w*h)\n        \n        out = torch.bmm(proj_value, attention)\n        out = out.view(batch_size, c, h, w)\n        \n        out = self.gamma*out + x\n        return out, attention\n\n    \nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            nn.Conv2d(\n                in_channels=channels,\n                out_channels=channels,\n                kernel_size=1,\n                stride=1,\n                padding=0))\n\n    def forward(self, x):\n        return self.block(x) + x\n    \n    \nclass Generator(nn.Module):\n    def __init__(self, img_channels):\n        super().__init__()\n        \n        self.downConvBlock = nn.Sequential((\n            nn.Conv2d(in_channels=img_channels, out_channels=1, kernel_size=7, stride=1, padding=3),\n            nn.Conv2d(in_channels=1, out_channels=64, kernel_size=6, stride=2, padding=2, bias=False),\n            nn.InstanceNorm2d(64),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(128),\n            nn.ReLU(),\n            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, bias=False),\n            nn.InstanceNorm2d(256),\n            nn.ReLU()))\n        self.convtrans1 = nn.ConvTranspose2d(in_channels=256, out_channels=128, kernel_size=3, stride=2, padding=1, bias=False)\n        self.active1 = nn.Sequential(nn.InstancNorm2d(128), nn.ReLU())\n        self.convtrans2 = nn.ConvTranspose2d(in_channels=128, out_channels=64, kernel_size=3, stride=2, padding=1)\n        self.convtrans3 = nn.ConvTranspose2d(in_channels=64, out_channels=1, kernel_size=6, stride=2, padding=2)\n        self.selfAttentionayer = SelfAttentionBlock(in_dim=256)\n        self.resBlock1 = ResidualBlock(channels=256)\n        self.resBlock2 = ResidualBlock(channel=256)\n        self.lastlayer = nn.Sequential(nn.Conv2d(in_channls=1, out_channels=1, kernel_size=7, stride=1, padding=3), nn.Tanh())\n            \n    def forward(self, x):\n        batch_size, ch, h, w = x.shape\n        out = self.downConvBlock(x)\n        out = self.resBlock1(out)\n        out, _ = self.AttentionLayer(out)\n        out = self.resBlock2(out)\n        out = self.convtrans1(out, output_size=(batch_size, 128, 32, 32))\n        out = self.active1(out)\n        out = self.convtrans2(out, output_size=(batch_size, 64, 64, 64))\n        out = self.convtrans3(out, output_size=(batch_size, 1, 128, 128))\n        out = self.lastlayer(out)","metadata":{"execution":{"iopub.status.busy":"2022-04-22T22:04:01.616978Z","iopub.execute_input":"2022-04-22T22:04:01.618116Z","iopub.status.idle":"2022-04-22T22:04:01.644756Z","shell.execute_reply.started":"2022-04-22T22:04:01.618069Z","shell.execute_reply":"2022-04-22T22:04:01.644061Z"},"trusted":true},"execution_count":46,"outputs":[]},{"cell_type":"code","source":"# Discriminator\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=4,\n                stride=stride,\n                padding=1,\n                bias=True,\n                padding_mode=\"reflect\"\n            ),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLu(0.2)       \n        )\n        \n    def forward(self, x):\n        return self.conv(x)       \n        \n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.initial = nn.Sequential(\n        nn.Conv2d(\n            in_channels,\n            features[0],\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            padding_mode=\"reflect\"\n        ),\n        nn.LeakyReLU(0.2))\n    \n        layers = []\n    \n        in_channels = features[0]\n    \n        for feature in features[1:]:\n            layers.append(\n                Block(in_channels, feature, stride=1 if feature==features[-1] else 2)\n            )\n            in_channels = feature\n        \n        layers.append(\n            nn.Conv2d(\n                in_channels,\n                1,\n                kernel_size=4,\n                stride=1,\n                padding=1,\n                padding_mode=\"reflect\"\n            ))\n        self.model = nn.Sequential(*layers)\n    \n    def forward():\n        x = self.initial(x)\n        return torch.sigmoid(self.model(x))","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:34:05.491689Z","iopub.execute_input":"2022-04-22T21:34:05.492221Z","iopub.status.idle":"2022-04-22T21:34:05.504550Z","shell.execute_reply.started":"2022-04-22T21:34:05.492186Z","shell.execute_reply":"2022-04-22T21:34:05.503807Z"},"trusted":true},"execution_count":44,"outputs":[]},{"cell_type":"code","source":"# utility\n\ndef save_some_examples_g(gen, val_loader, epoch, idx, folder):\n    img_glasses, img_no_glasses = next(iter(val_loader))\n    img_glasses, img_no_glasses = img_glasses.to(DEVICE), img_no_glasses.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake_glasses = gen(img_no_glasses)\n        y_fake_glasses = y_fake_glasses*0.5 + 0.5\n        save_image(y_fake_glasses, os.path.join(folder, f\"{epoch}_{idx}_fake_glasses.png\"))\n\ndef save_some_examples_n(gen, val_loader, epoch, idx, folder):\n    img_glasses, img_no_glasses = next(iter(val_loader))\n    img_glasses, img_no_glasses = img_gasses.to(DEVICE), img_no_glasses.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake_no_glasses = gen(img_glasses)\n        y_fake_no_glasses = y_fake_no_glasses*0.5 + 0.5\n        save_image(y_fake_no_glasses, os.path.join(folder, f\"{epoch}_{idx}_fake_no_glasses.png\"))\n\ndef save_checkpoint(model, optimizer, epoch, filename):\n    filename = str(epoch) + filename + \"_cpt.pth.tar\"\n    print(\"=> Saving checkpoint\")\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    print(\"=> Loading Checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state(checkpoint[\"state_dict\"])\n    optimizer.load_state(checkpoint[\"state_dict\"])\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n","metadata":{"execution":{"iopub.status.busy":"2022-04-22T21:34:09.498041Z","iopub.execute_input":"2022-04-22T21:34:09.498329Z","iopub.status.idle":"2022-04-22T21:34:09.510625Z","shell.execute_reply.started":"2022-04-22T21:34:09.498294Z","shell.execute_reply":"2022-04-22T21:34:09.509763Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"# train\n\ndef getVggFeatures():\n    pass\n\ndef pcclT1():\n    pass\n\ndef pcclT2():\n    pass\n\ndef trainFn():\n    pass","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# Instantiating Generators and Discriminators\ndisc_G = Discriminator(inp_channels=1).to(DEVICE)\ndisc_N = Discriminator(inp_channels=1).to(DEVICE)\ngen_G = Generator(img_channels=1, num_residuals=4).to(DEVICE)\ngen_N = Generator(img_channels=1, num_residuals=4).to(DEVICE)\n\n# Initializing optimizers\nopt_disc = optim.Adam(list(disc_G.parameters()) + list(disc_N.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\nopt_gen = optim.Adam(list(gen_G.parameters()) + list(gen_N.parameters()), lr = LEARNING_RATE, betas=(0.5, 0.999))\n\n# Loss Function\nL1 = nn.L1Loss()\nmse = nn.MSELoss()\n\n# Dataset Instantiation\ntrain_data = Dataset( train_imgs_glasses, train_imgs_no_glasses, transform = transforms_train)\nval_data = Dataset(test_imgs_glasses, test_imgs_no_glasses, transform = transforms_val )\n\n# Dataloaders\ntrain_loader = DataLoader(train_data, BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, BATCH_SIZE, shuffle=False)\n\n# Training Loop\nfor epoch in range(NUM_EPOCH):\n    print(f\"Epoch: {epoch}\")\n    trainFn(disc_G, disc_N, gen_G, gen_N, train_loader, opt_gen, opt_disc, L1, mse, val_loader, epoch)\n    \n    save_checkpoint(disc_G, opt_disc, epoch, \"disc_G\")\n    save_checkpoint(disc_N, opt_disc, epoch, \"disc_N\")\n    save_checkpoint(gen_G, opt_gen, epoch, \"gen_G\")\n    save_checkpoint(gen_N, opt_gen, epoch, \"gen_N\")","metadata":{"execution":{"iopub.status.busy":"2022-04-22T15:28:11.158450Z","iopub.execute_input":"2022-04-22T15:28:11.159170Z","iopub.status.idle":"2022-04-22T15:28:11.169791Z","shell.execute_reply.started":"2022-04-22T15:28:11.159128Z","shell.execute_reply":"2022-04-22T15:28:11.169059Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}