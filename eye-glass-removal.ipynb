{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/yashikajain/eye-glass-removal?scriptVersionId=95157787\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"code","source":"# Imports\n\nimport os\nimport numpy as np\nimport pandas as pd\nimport random\nimport torch\nimport torch.nn as nn\nfrom tqdm import tqdm\nimport torch.optim as optim\nfrom torchvision.utils import save_image\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom PIL import Image, ImageOps\nimport time\n\ntrain = pd.read_csv('../input/glasses-or-no-glasses/train.csv')\ntest = pd.read_csv('../input/glasses-or-no-glasses/test.csv')\ntrain.set_index('id', inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:13.102218Z","iopub.execute_input":"2022-05-09T12:13:13.10278Z","iopub.status.idle":"2022-05-09T12:13:17.406535Z","shell.execute_reply.started":"2022-05-09T12:13:13.102689Z","shell.execute_reply":"2022-05-09T12:13:17.405645Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# configs \nBATCH_SIZE = 1\nLAMBDA_IDENTITY = 0.0\nLEARNING_RATE = 2e-4\nNUM_EPOCHS = 2\nLAMBDA_CYCLE = 10 \nDEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:17.408843Z","iopub.execute_input":"2022-05-09T12:13:17.409433Z","iopub.status.idle":"2022-05-09T12:13:17.47328Z","shell.execute_reply.started":"2022-05-09T12:13:17.409391Z","shell.execute_reply":"2022-05-09T12:13:17.472325Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"def set_seed(seed):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    if torch.cuda.is_available():\n        torch.cuda.manual_seed_all(seed)\n        torch.backends.cudnn.deterministic = True\n\nset_seed(42)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:17.475088Z","iopub.execute_input":"2022-05-09T12:13:17.475386Z","iopub.status.idle":"2022-05-09T12:13:17.488311Z","shell.execute_reply.started":"2022-05-09T12:13:17.47534Z","shell.execute_reply":"2022-05-09T12:13:17.487191Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Output Directories\nos.mkdir('./NoGlasses')\nos.mkdir('./Glasses')\n\nresults_NoGlasses = './NoGlasses'\nresults_Glasses = './Glasses'","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:17.491277Z","iopub.execute_input":"2022-05-09T12:13:17.491707Z","iopub.status.idle":"2022-05-09T12:13:17.49919Z","shell.execute_reply.started":"2022-05-09T12:13:17.49167Z","shell.execute_reply":"2022-05-09T12:13:17.498184Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# segregating images with and without glasses\ndirectory = '../input/glasses-or-no-glasses/faces-spring-2020/faces-spring-2020'\ntrain_imgs_glasses = []\ntrain_imgs_no_glasses = []\ntest_imgs = []\nfor img in os.listdir(directory):\n    img_id = int(img.replace('face-', ''  ).replace('.png', ''))\n    for i in range(len(train)+1):\n        if (i) == (img_id):\n            if train.loc[img_id]['glasses'] == 0:\n                train_imgs_no_glasses.append(os.path.join(directory, str(img)))\n                break\n                \n            elif train.loc[img_id]['glasses'] == 1:\n                train_imgs_glasses.append(os.path.join(directory, str(img)))\n                break\n                \n        \n    for i in range(len(test)+1): \n        if int(i) == int(img_id):\n            test_imgs.append(os.path.join(directory, str(img)))\n            break         ","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:17.501054Z","iopub.execute_input":"2022-05-09T12:13:17.501534Z","iopub.status.idle":"2022-05-09T12:13:23.340914Z","shell.execute_reply.started":"2022-05-09T12:13:17.501492Z","shell.execute_reply":"2022-05-09T12:13:23.3401Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"print(len(train_imgs_glasses))\nprint(len(train_imgs_no_glasses))\nprint(len(test_imgs))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.34229Z","iopub.execute_input":"2022-05-09T12:13:23.342584Z","iopub.status.idle":"2022-05-09T12:13:23.349839Z","shell.execute_reply.started":"2022-05-09T12:13:23.342547Z","shell.execute_reply":"2022-05-09T12:13:23.34879Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stdout","text":"2856\n1644\n500\n","output_type":"stream"}]},{"cell_type":"code","source":"test_imgs_glasses = train_imgs_glasses[:571]\ntest_imgs_no_glasses = train_imgs_no_glasses[:328]\ntrain_imgs_glasses = train_imgs_glasses[571:]\ntrain_imgs_no_glasses = train_imgs_no_glasses[328:]","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.351346Z","iopub.execute_input":"2022-05-09T12:13:23.351787Z","iopub.status.idle":"2022-05-09T12:13:23.359569Z","shell.execute_reply.started":"2022-05-09T12:13:23.351745Z","shell.execute_reply":"2022-05-09T12:13:23.35878Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"class Dataset(Dataset):\n    def __init__(self, imgs_glasses, imgs_no_glasses, transform = None ):\n        super().__init__()\n        self.root_dir = directory\n        self.glasses_list = imgs_glasses\n        self.no_glasses_list = imgs_no_glasses\n        self.transform = transform\n        \n    def __len__(self):\n        self.glasses_len = len(self.glasses_list)\n        self.no_glasses_len = len(self.no_glasses_list)\n        self.dataset_len = max(self.glasses_len, self.no_glasses_len)\n        return self.dataset_len\n    \n    def __getitem__(self, idx):\n        glasses_path = self.glasses_list[idx % self.glasses_len]\n        no_glasses_path = self.no_glasses_list[idx % self.no_glasses_len]\n        img_glasses = np.array(ImageOps.grayscale(Image.open(glasses_path).convert('RGB')))\n        img_no_glasses = np.array(ImageOps.grayscale(Image.open(no_glasses_path).convert('RGB')))\n        \n        if self.transform:\n            augmentation = self.transform(image=img_glasses, image0=img_no_glasses)\n            img_glasses = augmentation[\"image0\"]\n            img_no_glasses = augmentation[\"image\"]\n            \n            \n        return img_glasses, img_no_glasses","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.361092Z","iopub.execute_input":"2022-05-09T12:13:23.361514Z","iopub.status.idle":"2022-05-09T12:13:23.374494Z","shell.execute_reply.started":"2022-05-09T12:13:23.361473Z","shell.execute_reply":"2022-05-09T12:13:23.373622Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"transforms_train = A.Compose(\n    [\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5], std=[0.5], max_pixel_value=255.0),\n        ToTensorV2()\n    ],\n    additional_targets = {\"image0\":\"image\"}\n)\n\ntransforms_val = A.Compose(\n    [\n        A.Resize(256, 256),\n        A.HorizontalFlip(p=0.5),\n        A.Normalize(mean=[0.5], std=[0.5], max_pixel_value=255.0),\n        ToTensorV2()\n    ],\n    additional_targets = {\"image0\":\"image\"}\n)","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.377179Z","iopub.execute_input":"2022-05-09T12:13:23.378071Z","iopub.status.idle":"2022-05-09T12:13:23.385735Z","shell.execute_reply.started":"2022-05-09T12:13:23.37801Z","shell.execute_reply":"2022-05-09T12:13:23.384845Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Generator\n\nclass ConvBlock(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, use_act=True, **kwargs):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, padding_mode=\"reflect\", **kwargs)\n            if down\n            else nn.ConvTranspose2d(in_channels, out_channels, **kwargs),\n            nn.InstanceNorm2d(out_channels),\n            nn.ReLU(inplace=True) if use_act else nn.Identity()\n        )\n\n    def forward(self, x):\n        return self.conv(x)\n\n    \nclass ResidualBlock(nn.Module):\n    def __init__(self, channels):\n        super().__init__()\n        self.block = nn.Sequential(\n            ConvBlock(channels, channels, kernel_size=3, padding=1),\n            ConvBlock(channels, channels, use_act=False, kernel_size=3, padding=1),\n        )\n\n    def forward(self, x):\n        return x + self.block(x)\n    \nclass Generator(nn.Module):\n    def __init__(self, img_channels, num_features = 64, num_residuals=9):\n        super().__init__()\n        self.initial = nn.Sequential(\n            nn.Conv2d(img_channels, num_features, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\"),\n            nn.InstanceNorm2d(num_features),\n            nn.ReLU(inplace=True),\n        )\n        self.down_blocks = nn.ModuleList(\n            [\n                ConvBlock(num_features, num_features*2, kernel_size=3, stride=2, padding=1),\n                ConvBlock(num_features*2, num_features*4, kernel_size=3, stride=2, padding=1),\n            ]\n        )\n        self.res_blocks = nn.Sequential(\n            *[ResidualBlock(num_features*4) for _ in range(num_residuals)]\n        )\n        self.up_blocks = nn.ModuleList(\n            [\n                ConvBlock(num_features*4, num_features*2, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n                ConvBlock(num_features*2, num_features*1, down=False, kernel_size=3, stride=2, padding=1, output_padding=1),\n            ]\n        )\n\n        self.last = nn.Conv2d(num_features*1, img_channels, kernel_size=7, stride=1, padding=3, padding_mode=\"reflect\")\n\n    def forward(self, x):\n        x = self.initial(x)\n        for layer in self.down_blocks:\n            x = layer(x)\n        x = self.res_blocks(x)\n        for layer in self.up_blocks:\n            x = layer(x)\n        return torch.tanh(self.last(x))    ","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.389908Z","iopub.execute_input":"2022-05-09T12:13:23.39037Z","iopub.status.idle":"2022-05-09T12:13:23.407488Z","shell.execute_reply.started":"2022-05-09T12:13:23.390325Z","shell.execute_reply":"2022-05-09T12:13:23.406659Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Discriminator\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, stride):\n        super().__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(\n                in_channels,\n                out_channels,\n                kernel_size=4,\n                stride=stride,\n                padding=1,\n                bias=True,\n                padding_mode=\"reflect\"\n            ),\n            nn.InstanceNorm2d(out_channels),\n            nn.LeakyReLU(0.2)       \n        )\n        \n    def forward(self, x):\n        return self.conv(x)       \n        \n\nclass Discriminator(nn.Module):\n    def __init__(self, in_channels, features=[64, 128, 256, 512]):\n        super().__init__()\n        self.initial = nn.Sequential(\n        nn.Conv2d(\n            in_channels,\n            features[0],\n            kernel_size=4,\n            stride=2,\n            padding=1,\n            padding_mode=\"reflect\"\n        ),\n        nn.LeakyReLU(0.2))\n    \n        layers = []\n    \n        in_channels = features[0]\n    \n        for feature in features[1:]:\n            layers.append(\n                Block(in_channels, feature, stride=1 if feature==features[-1] else 2)\n            )\n            in_channels = feature\n        \n        layers.append(\n            nn.Conv2d(\n                in_channels,\n                1,\n                kernel_size=4,\n                stride=1,\n                padding=1,\n                padding_mode=\"reflect\"\n            ))\n        self.model = nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.initial(x)\n        return torch.sigmoid(self.model(x))","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:13:23.409556Z","iopub.execute_input":"2022-05-09T12:13:23.411379Z","iopub.status.idle":"2022-05-09T12:13:23.424037Z","shell.execute_reply.started":"2022-05-09T12:13:23.411332Z","shell.execute_reply":"2022-05-09T12:13:23.423303Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# utility\n\ndef save_some_examples_g(gen, val_loader, epoch, idx, folder):\n    img_glasses, img_no_glasses = next(iter(val_loader))\n    img_glasses, img_no_glasses = img_glasses.to(DEVICE), img_no_glasses.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake_glasses = gen(img_no_glasses)\n        y_fake_glasses = y_fake_glasses*0.5 + 0.5\n        save_image(y_fake_glasses, os.path.join(folder, f\"{epoch}_{idx}_fake_glasses.png\"))\n\ndef save_some_examples_n(gen, val_loader, epoch, idx, folder):\n    img_glasses, img_no_glasses = next(iter(val_loader))\n    img_glasses, img_no_glasses = img_glasses.to(DEVICE), img_no_glasses.to(DEVICE)\n    gen.eval()\n    with torch.no_grad():\n        y_fake_no_glasses = gen(img_glasses)\n        y_fake_no_glasses = y_fake_no_glasses*0.5 + 0.5\n        save_image(y_fake_no_glasses, os.path.join(folder, f\"{epoch}_{idx}_fake_no_glasses.png\"))\n\ndef save_checkpoint(model, optimizer, epoch, filename):\n    filename = str(epoch) + filename + \"_cpt.pth.tar\"\n    #print(\"=> Saving checkpoint\")\n    checkpoint = {\"state_dict\": model.state_dict(), \"optimizer\": optimizer.state_dict()}\n    torch.save(checkpoint, filename)\n\ndef load_checkpoint(checkpoint_file, model, optimizer, lr):\n    #print(\"=> Loading Checkpoint\")\n    checkpoint = torch.load(checkpoint_file, map_location=DEVICE)\n    model.load_state(checkpoint[\"state_dict\"])\n    optimizer.load_state(checkpoint[\"state_dict\"])\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr\n","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:19:28.499698Z","iopub.execute_input":"2022-05-09T12:19:28.500009Z","iopub.status.idle":"2022-05-09T12:19:28.510492Z","shell.execute_reply.started":"2022-05-09T12:19:28.499978Z","shell.execute_reply":"2022-05-09T12:19:28.509238Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# train\ndef trainFn(disc_G, disc_N, gen_G, gen_N, loader, optim_g, optim_d, l1, mse, val_loader, epoch, epoch_generator_loss, epoch_discriminator_loss):\n    loop = tqdm(loader,leave=True, position=0)\n    for idx, (img_glasses, img_no_glasses) in enumerate(loop):\n      \n        img_glasses = img_glasses.to(DEVICE)\n        img_no_glasses = img_no_glasses.to(DEVICE)\n        \n        fake_no_glasses = gen_N(img_glasses)\n        disc_ng_real = disc_N(img_no_glasses)\n        disc_ng_fake = disc_N(fake_no_glasses.detach())\n        disc_ng_real_loss = mse(disc_ng_real, torch.ones_like(disc_ng_real))\n        disc_ng_fake_loss = mse(disc_ng_fake, torch.zeros_like(disc_ng_fake))\n        disc_ng_loss = disc_ng_fake_loss + disc_ng_real_loss\n            \n        \n        fake_glasses = gen_G(img_no_glasses)\n        disc_g_real = disc_G(img_glasses)\n        disc_g_fake = disc_G(fake_glasses.detach())\n        disc_g_real_loss = mse(disc_g_real, torch.ones_like(disc_g_real))\n        disc_g_fake_loss = mse(disc_g_fake, torch.zeros_like(disc_g_fake))\n        disc_g_loss = disc_g_real_loss + disc_g_fake_loss\n        \n        #total disc loss\n        D_loss = (disc_ng_loss + disc_g_loss)/2\n        \n        # backprop and update the weights of discriminator\n        optim_d.zero_grad()\n        D_loss.backward()\n        optim_d.step()\n        \n        # Training Generators\n        \n        # Adversarial loss\n        disc_g_fake = disc_G(fake_glasses)\n        disc_ng_fake = disc_N(fake_no_glasses)\n        loss_g_glasses = mse(disc_g_fake, torch.ones_like(disc_g_fake))\n        loss_g_no_glasses = mse(disc_ng_fake, torch.ones_like(disc_ng_fake))\n        \n        # Cycle loss\n        cycle_glasses = gen_G(fake_no_glasses)\n        cycle_no_glasses = gen_N(fake_glasses)\n        glasses_cycle_loss = l1(img_glasses, cycle_glasses)\n        no_glasses_cycle_loss = l1(img_no_glasses, cycle_no_glasses)\n    \n        \n        # Total Generator Loss\n        G_loss = (loss_g_glasses + loss_g_no_glasses) + LAMBDA_CYCLE*(glasses_cycle_loss + no_glasses_cycle_loss) #+ loss_pccl \n        \n        # backprop and updating weights of Generator\n        optim_g.zero_grad()\n        G_loss.backward()\n        optim_g.step()\n        \n        epoch_generator_loss = epoch_generator_loss + G_loss\n        epoch_discriminator_loss = epoch_discriminator_loss + D_loss\n        \n        #print(f\"Generator loss: {G_loss}, Discriminator loss: {D_loss}\")\n        save_some_examples_n(gen_N, val_loader, epoch, idx, folder=results_NoGlasses)\n        save_some_examples_g(gen_G, val_loader, epoch, idx, folder=results_Glasses)\n    return epoch_generator_loss, epoch_discriminator_loss\n        \n        \n    ","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:19:28.940372Z","iopub.execute_input":"2022-05-09T12:19:28.940909Z","iopub.status.idle":"2022-05-09T12:19:28.953622Z","shell.execute_reply.started":"2022-05-09T12:19:28.940876Z","shell.execute_reply":"2022-05-09T12:19:28.952924Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"\n# Instantiating Generators and Discriminators\ndisc_G = Discriminator(in_channels=1).to(DEVICE)\ndisc_N = Discriminator(in_channels=1).to(DEVICE)\ngen_G = Generator(img_channels=1).to(DEVICE)\ngen_N = Generator(img_channels=1).to(DEVICE)\n\n# Initializing optimizers\nopt_disc = optim.Adam(list(disc_G.parameters()) + list(disc_N.parameters()), lr=LEARNING_RATE, betas=(0.5, 0.999))\nopt_gen = optim.Adam(list(gen_G.parameters()) + list(gen_N.parameters()), lr = LEARNING_RATE, betas=(0.5, 0.999))\n\n# Loss Function\nL1 = nn.L1Loss()\nmse = nn.MSELoss()\n\n# Dataset Instantiation\ntrain_data = Dataset(imgs_glasses=train_imgs_glasses, imgs_no_glasses=train_imgs_no_glasses, transform=transforms_train)\nval_data = Dataset(imgs_glasses=test_imgs_glasses, imgs_no_glasses=test_imgs_no_glasses, transform = transforms_val )\n\n# Dataloaders\ntrain_loader = DataLoader(train_data, BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, BATCH_SIZE, shuffle=False)\n\n# Training Loop\nfor epoch in range(NUM_EPOCHS):\n    print(f\"Epoch: {epoch}\")\n    epoch_generator_loss = 0\n    epoch_discriminator_loss = 0\n    gen_loss, disc_loss = trainFn(disc_G, disc_N, gen_G, gen_N, train_loader, opt_gen, opt_disc, L1, mse, val_loader, epoch, epoch_generator_loss, epoch_discriminator_loss)\n    print(f'Generator_loss {gen_loss}')\n    print(f'Discriminator_loss {disc_loss}')\n\n    save_checkpoint(disc_G, opt_disc, epoch, \"disc_G\")\n    save_checkpoint(disc_N, opt_disc, epoch, \"disc_N\")\n    save_checkpoint(gen_G, opt_gen, epoch, \"gen_G\")\n    save_checkpoint(gen_N, opt_gen, epoch, \"gen_N\")","metadata":{"execution":{"iopub.status.busy":"2022-05-09T12:19:29.37611Z","iopub.execute_input":"2022-05-09T12:19:29.37668Z","iopub.status.idle":"2022-05-09T13:21:52.98556Z","shell.execute_reply.started":"2022-05-09T12:19:29.376638Z","shell.execute_reply":"2022-05-09T13:21:52.984413Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Epoch: 0\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2285/2285 [31:48<00:00,  1.20it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generator_loss 9724.5947265625\nDiscriminator_loss 591.7130737304688\nEpoch: 1\n","output_type":"stream"},{"name":"stderr","text":"100%|██████████| 2285/2285 [30:30<00:00,  1.25it/s]\n","output_type":"stream"},{"name":"stdout","text":"Generator_loss 9355.0146484375\nDiscriminator_loss 297.63946533203125\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}