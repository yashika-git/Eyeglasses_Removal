{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Discriminator\n# concatenated x and y because the input and output images are combined in a single image\n# also, that's why inp_channels*2 in Conv2d layer\nimport torch \nimport torch.nn as nn\nclass CNNBlock(nn.Module):\n    def __init__(self, in_channels, out_channels,stride=2):\n        super(CNNBlock, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, 4, stride, bias=False, padding=1, padding_mode='reflect'),\n        nn.BatchNorm2d(out_channels),\n        nn.LeakyReLU(0.2))\n        \n    def forward(self, x):\n        return self.conv(x)\n    \nclass Discriminator(nn.Module):\n    \n    def __init__(self, in_channels, features=[64,128,256,512]): #256 -> 30x30\n        super().__init__()\n        self.initial = nn.Sequential(\n        nn.Conv2d(in_channels*2,features[0], kernel_size=4, stride=2, padding=1, padding_mode='reflect'),\n        nn.LeakyReLU(0.2)\n        )\n        \n        layers = []\n        in_channels = features[0]\n        for feature in features[1:]:\n            layers.append(\n                CNNBlock(in_channels, feature, stride=1 if feature==features[-1] else 2),\n            )\n            in_channels = feature\n            \n        layers.append(\n            nn.Conv2d(\n                in_channels, 1, kernel_size=4, stride=1, padding=1, padding_mode=\"reflect\"\n            ),\n        )\n        self.model = nn.Sequential(*layers)    \n    \n    def forward(self, x, y):\n        x = torch.cat([x,y], dim=1)\n        x = self.initial(x)\n        return self.model(x)\n    \ndef test():\n    x = torch.randn((1, 3, 256, 256))\n    y = torch.randn((1, 3, 256, 256))\n    model = Discriminator(in_channels=3)\n    preds = model(x, y)\n    print(model)\n    print(preds.shape)\n\n\nif __name__ == \"__main__\":\n    test()\n    \n    ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-04-01T19:12:17.648132Z","iopub.execute_input":"2022-04-01T19:12:17.648956Z","iopub.status.idle":"2022-04-01T19:12:17.768313Z","shell.execute_reply.started":"2022-04-01T19:12:17.648890Z","shell.execute_reply":"2022-04-01T19:12:17.767417Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Discriminator(\n  (initial): Sequential(\n    (0): Conv2d(6, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), padding_mode=reflect)\n    (1): LeakyReLU(negative_slope=0.2)\n  )\n  (model): Sequential(\n    (0): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (1): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (2): CNNBlock(\n      (conv): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), bias=False, padding_mode=reflect)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (2): LeakyReLU(negative_slope=0.2)\n      )\n    )\n    (3): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), padding=(1, 1), padding_mode=reflect)\n  )\n)\ntorch.Size([1, 1, 30, 30])\n","output_type":"stream"}]},{"cell_type":"code","source":"# Generator\n# Similar to UNet\n# down specifies downward part of UNet\n# Encoder LeakyReLU, decoder ReLU\n# dont use batchnorm in initial layer\n# bias=False because we are using BatchNorm\n# output image of init has dimensions 128, as (n-f+2p/s)+1 => (256-4+2)/2 + 1 => 128\n\nimport torch \nimport torch.nn as nn\n\nclass Block(nn.Module):\n    def __init__(self, in_channels, out_channels, down=True, act='relu', use_dropout=False):\n        super(Block, self).__init__()\n        self.conv = nn.Sequential(\n        nn.Conv2d(in_channels, out_channels, kernel_size=4, stride=2, padding=1, bias=False, padding_mode='reflect')\n        if down\n        else nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n            nn.BatchNorm2d(out_channels),\n            nn.ReLU() if act =='relu' else nn.LeakyReLU(0.2),   \n        )\n        \n        self.use_dropout = use_dropout\n        self.dropout = nn.Dropout(0.5)\n        self.down = down\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return self.dropout(x) if self.use_dropout else x\n\nclass Generator(nn.Module):\n    def __init__(self, in_channels=3, features=64):\n        super().__init__()\n        self.initial_down = nn.Sequential(\n            nn.Conv2d(in_channels,features, 4, 2, 1, padding_mode='reflect' ),\n            nn.LeakyReLU(0.2))\n        self.down1 = Block(features, features*2, down=True, act='leaky', use_dropout=False) #64x64\n        self.down2 = Block(features*2, features*4, down=True, act='leaky', use_dropout=False) #32x32\n        self.down3 = Block(features*4, features*8, down=True, act='leaky', use_dropout=False) #16x16\n        self.down4 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #8x8\n        self.down5 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #4x4\n        self.down6 = Block(features*8, features*8, down=True, act='leaky', use_dropout=False) #2x2\n        \n        self.bottleneck = nn.Sequential(nn.Conv2d(features*8, features*8, 4, 2, 1), nn.ReLU()) #1x1\n        \n        self.up1 = Block(features*8, features*8, down=False, act='relu', use_dropout=True)\n        self.up2 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up3 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up4 = Block(features*8*2, features*8, down=False, act='relu', use_dropout=True)\n        self.up5 = Block(features*8*2, features*4, down=False, act='relu', use_dropout=True)\n        self.up6 = Block(features*4*2, features*2, down=False, act='relu', use_dropout=True)\n        self.up7 = Block(features*2*2, features, down=False, act='relu', use_dropout=True)\n        self.final_up = nn.Sequential(nn.ConvTranspose2d(features*2, in_channels, kernel_size=4, stride=2, padding=1),\n            nn.Tanh(),)\n        \n    def forward(self, x):\n        d1 = self.initial_down(x)\n        d2 = self.down1(d1)\n        d3 = self.down2(d2)\n        d4 = self.down3(d3)\n        d5 = self.down4(d4)\n        d6 = self.down5(d5)\n        d7 = self.down6(d6)\n        bottleneck = self.bottleneck(d7)\n        up1 = self.up1(bottleneck)\n        up2 = self.up2(torch.cat([up1, d7], 1))\n        up3 = self.up3(torch.cat([up2, d6], 1))\n        up4 = self.up4(torch.cat([up3, d5], 1))\n        up5 = self.up5(torch.cat([up4, d4], 1))\n        up6 = self.up6(torch.cat([up5, d3], 1))\n        up7 = self.up7(torch.cat([up6, d2], 1))\n        return self.final_up(torch.cat([up7, d1], 1))\ndef test():\n    x = torch.randn((1, 3, 256, 256))\n    model = Generator(in_channels=3, features=64)\n    preds = model(x)\n    print(preds.shape)\n\n\nif __name__ == \"__main__\":\n    test()","metadata":{"execution":{"iopub.status.busy":"2022-04-02T08:24:34.187584Z","iopub.execute_input":"2022-04-02T08:24:34.188349Z","iopub.status.idle":"2022-04-02T08:24:34.954513Z","shell.execute_reply.started":"2022-04-02T08:24:34.188288Z","shell.execute_reply":"2022-04-02T08:24:34.953662Z"},"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"torch.Size([1, 3, 256, 256])\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}
